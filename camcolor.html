<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="Mark Otto, Jacob Thornton, and Bootstrap contributors">
    <meta name="generator" content="Hugo 0.88.1">
    <title>Exploring Camera Color Space</title>

    <link rel="canonical" href="https://getbootstrap.com/docs/5.1/examples/headers/">

    <!-- Bootstrap core CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-F3w7mX95PdgyTmZZMECAngseQB83DfGTowi0iMjiWaeVhAn4FJkqJByhZMI3AhiU" crossorigin="anonymous">


    <style>
      .bd-placeholder-img {
        font-size: 1.125rem;
        text-anchor: middle;
        -webkit-user-select: none;
        -moz-user-select: none;
        user-select: none;
      }

      @media (min-width: 768px) {
        .bd-placeholder-img-lg {
          font-size: 3.5rem;
        }
      }
    </style>

    
    <!-- Custom styles for this template -->
    <link href="headers.css" rel="stylesheet">

    <!-- JavaScript -->
    <script src='https://cdn.plot.ly/plotly-2.4.2.min.js'></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjs/3.2.1/math.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/Chart.js/3.0.2/chart.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/chartjs-plugin-zoom/1.1.1/chartjs-plugin-zoom.min.js"></script>
    <script src="https://d3js.org/d3.v3.min.js"></script>
    <script src='https://cdnjs.cloudflare.com/ajax/libs/jquery/3.1.1/jquery.min.js'></script>
    <script src='hull.js'></script>

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {
            inlineMath: [['$','$'], ['\\(','\\)']],
            displayMath: [ ['$$','$$'], ['\[','\]'] ]
          }
        });
        MathJax.Hub.Register.StartupHook("End",function () {
          console.log("Mathjax loaded");
          console.log(typeof MathJax);
          MathJax.Hub.Queue(["Typeset", MathJax.Hub, "TFS"]);
        });
    </script>
  </head>
  <body>

<main>
  <div class="container">
    <header class="d-flex flex-wrap justify-content-center pt-3 mb-4 border-bottom">
      <a href="." class="d-flex align-items-center mb-3 mb-md-0 me-md-auto text-dark text-decoration-none">
        <h2>Exploring Camera Color Space and Color Correction</h2>
      </a>
    </header>

    <div class="d-grid gap-3 pt-3 mb-4 px-4 bg-light border rounded-3">
      <div class="row">
        <h3>Introduction</h3>
        <p>Do cameras see the same color as us? Can cameras always accurately reproduce colors that our eyes see? This interactive tutorial explores these questions and many more interesting aspects of camera raw color space. In particular, we will walk you through an important concept in both color science and camera signal processing: color correction, the process of correcting the color perception of a camera such that it is as close to ours as allowed. In the end, you will get to appreciate why you should never trust the color produced by your camera and how you might build your own camera that, in theory, out-performs existing cameras in color reproduction.</p>
        <p><b>Caveats</b>. 1) This tutorial demonstrates the principle of color correction with many important, but subtle, engineering details omitted; we will mention them when appropriate. 2) Color correction is one of the two components in camera color reproduction, the other being white balance (or rather, camera's emulation of chromatic adaption of human visual system). We have a <a href="http://yuhaozhu.com/blog/chromatic-adaptation.html">post</a> that discusses the principles of chromatic adaptation and its application in white balance. The relationship of color correction and white balance is quite tricky, but Andrew Rowlands has a <a href="https://www.spiedigitallibrary.org/journals/optical-engineering/volume-59/issue-11/110801/Color-conversion-matrices-in-digital-cameras-a-tutorial/10.1117/1.OE.59.11.110801.full?SSO=1">fascinating article</a> that demystefies it for you.</p>
      </div>
    </div>

    <div class="d-grid gap-3 pt-3 mb-4 px-4 bg-light border rounded-3">
      <div class="row">
        <h3>Step 1: Exploring Camera Color Space</h3>
        <p>In principle, cameras work just like our eyes. Our retina has three types of cones (L, M, and S), each with a unique spectral sensitivity, translating light into three numbers (the L, M, S cone responses) that give us color perception. Similar to the three cone types, (most) cameras use three color filters (commonly referred to as the R, G, and B filters), each with a unique spectral sensitivity and, thus, also translate light into three numbers. In this sense, you can really think of a camera as a "weird" kind of human being with unconventional LMS cone fundamentals. Not all cameras use three filters though. Telescope imaging cameras use <a href="https://www.sdss.org/instruments/camera/">five filters</a>, just like <a href="https://phys.org/news/2013-09-mantis-shrimp-world-eyesbut.html">butterflies</a>!</p>
        <p>The left chart below shows the measured camera sensitivity functions of 48 cameras in two <a href="https://www.gujinwei.org/research/camspec/">recent</a> <a href="https://www.mdpi.com/1424-8220/21/15/4985/htm">studies</a>. The 48 cameras are classified into four categories: DSLR, point and shoot, industrial cameras, and smartphone cameras. The sensitivities are normalized such that the most sensitive filter (usually the green filter) peaks at 1. What should be noted is that the sensitivities functions measured here are not just the spectral transmittances of the color filters; rather, they are measured by treating the camera as a black box, and thus reflect the combined effects of everything in the camera that has a spectral response to light, such as the anti-aliasing filter, IR filter, micro-lenses, the photosites, etc.</p>
        <p> The default view shows the average sensitivities across the 48 cameras, but you can also select a particular camera from the drop-down list. As a comparison, we also plot the LMS cone fundamentals in the same chart as dashed lines. As is customary, the LMS cone fundamentals are, each, normalized to peak at 1. As is usually the case in color science, these normalizations merely introduce some scaling factors that will be canceled out later if we care about just the chromaticity of a color (i.e., the relative ratio of the primaries).</p>
      </div>

      <div class="row">
        <div class="col-sm-7">
          <select id="camSel" style="width: 150px" class="form-select">
          </select>
          <canvas id="canvasCamSpace" class="mt-2"></canvas>
        </div>
        <div class="col-sm-5">
          <div id="locusDiv"></div>
        </div>
      </div>

      <div class="row">
        <div class="col-sm-7">
          <div class="d-flex justify-content-start"><button id="resetChartCam" class="btn btn-primary btn-sm" disabled>Reset Custom Camera</button> </div>
        </div>
        <div class="col-sm-5">
          <div class="d-flex justify-content-end"><button id="correctLocus" class="btn btn-primary btn-sm" disabled>Draw Corrected Locus</button> </div>
        </div>
      </div>

      <div class="row">
        <p>You can see that the shapes of the camera sensitivities functions more or less resemble those of the cone fundamentals, which is perhaps not all that surprising. After all, cameras are built to reproduce colors that our eyes see. The shapes, however, are not an exact match. Most notably, you will see that the L and M cone responses overlap much more than the R and the G filters overlap. As you can imagine, sensitivities that are overly close to each other won't provide a great ability to distinguish colors. In the extreme case where two filters' sensitivities exactly match, the camera is dichromatic.</p>
        <!-- The camera sensitivities functions rarely exactly mimic the LMS cone responses; they are limited by the optical and silicon materials, the fabrication process, and have to consider not only color reproduction but also the signal-to-noise ratio. -->
        <p>Although by default disabled, the chart also contains the CIE 1931 XYZ Color Matching Functions (CMFs). You can enable them by clicking on the legend to the left of the chart. In fact, clicking on a legend label toggles the visibility of the corresponding curve in the chart. The XYZ CMFs are just one linear transformation away from the LMS cone fundamentals, and is used as the "common language" in colorimetry when comparing different color spaces. So we will use XYZ as the connection color space in the rest of the tutorial.</p>
        <p>In the same way we can plot the spectral locus in the XYZ color space, we can also plot the spectral locus on a camera's native RGB color space. The 3D plot on the right shows you the spectral locus on the XYZ, LMS, and the camera's RGB color space. The LMS locus is by default disabled, but you can enable it by clicking its legend label. Drag your mouse and spin around the 3D plot. You can see that the spectral locus in XYZ and in camera's RGB space do not match.</p>
        <p>You can also draw your own camera sensitivity functions. Select "Custom" from the drop-down list, and start dragging your mouse. As you draw, the spectral locus in the camera RGB space will be dynamically updated on the 3D plot on the right. You can hit the Draw Reset button to clean the drawing.</p>
      </div>
    </div>

    <div class="d-grid gap-3 pt-3 mb-4 px-4 bg-light border rounded-3">
      <div class="row">
        <h3>Step 2: The Correction Targets</h3>
        <p>The fact that the locus in XYZ and in the RGB locus do not match means that each spectral light has different XYZ and RGB tristimulus values. That in itself is not a problem: the LMS and the XYZ tristimulus values of spectral lights don't match either. Critically, however, the LMS and XYZ color spaces are just one linear transformation away from each other. Any light, not just spectral light, can be converted between the XYZ and the LMS space using one single $3\times 3$ matrix multiplication. So it really doesn't matter in which space you express the color of a light; whether in LMS or XYZ (or any other colorimetric space), it's all the same underlying color.</p>
        <p>Here comes the central question of this tutorial: is a camera's RGB color space also just a linear transformation from the XYZ/LMS space? If so, then the raw camera RGB values of any light can be translated to the correct XYZ values of that light, essentially recovering the color of the light from camera captures. In other words, the camera sees the same colors as us, and metamers to our eyes are also metamers to the camera. In general, if the camera raw color space is precisely a linear transformation away from the XYZ color space, the camera is said to satisfy the <a href="https://en.wikipedia.org/wiki/Tristimulus_colorimeter#:~:text=A%20camera%20or%20colorimeter%20is,of%20the%20filters%20is%20a">Luther Condition</a> and that the camera color space is colorimetric (in that we can use the camera to measure color). What would happen if a camera doesn't satisty the Luther Condition? Well, lights that are different to us (i.e., have different XYZ values) might be the same to the camera (i.e., have the same raw RGB values), and vice versa. Metamers to the camera would not be metameras to our eyes.</p>
        <p>Stated mathematically, we want to find a $3\times 3$ transformation matrix $T$ that satifies the following equation:</p>
        <p>
        $
        \begin{bmatrix}
        X_0 & X_1 & X_2 & \dots \\
        Y_0 & Y_1 & Y_2 & \dots \\
        Z_0 & Z_1 & Z_2 & \dots
        \end{bmatrix} =
        \begin{bmatrix}
        T_{00} & T_{01} & T_{02} \\
        T_{10} & T_{11} & T_{12} \\
        T_{20} & T_{21} & T_{22}
        \end{bmatrix} \times
        \begin{bmatrix}
        R_0 & R_1 & R_2 & \dots \\
        G_0 & G_1 & G_2 & \dots \\
        B_0 & B_1 & B_2 & \dots
        \end{bmatrix},
        $
        </p>
        <p>where $[X_i, Y_i, Z_i]^T$ is the XYZ values of a light and $[R_i, G_i, B_i]^T$ is the camera raw RGB values of the same light. Since the transformation matrix $T$ has 9 unknowns but $T$ should ideally work for any arbitrary light, i.e., infinitely many equations, the system of equations is over-determined. Instead of finding an exact solution, we will instead settle for a <i>best-fit</i> $T$ that works well for a set of lights whose color reproduction we care about.</p>
      </div>

      <div class="row">
        <canvas id="canvasCCSpec"></canvas>
      </div>

      <div class="row mt-2">
        <p>What are the lights we care to to reproduct their colors? We could certainly just use the spectral lights in the chart above, or could even just pick some random lights. But spectral lights are not commonly seen in real-world. In practical camera color calibration, what's often used is what's called the <a href="https://en.wikipedia.org/wiki/ColorChecker">ColorChecker color redition chart</a>, which contains 24 patches whose "spectral reflectances mimic those of natural objects such as human skin, foliage, and flowers." These are perfect target lights for us to calibrate the transformation matrix. While the original manufacturer <a href="https://www.xrite.com/service-support/faq_colorchecker_sg_spectral_data">does not publish</a> the spectral data, people have measured and published the spectral reflectance of these patches. The ones we plot above are from <a href="https://www.babelcolor.com/colorchecker-2.htm#CCP2_data">BabelColor</a>, but you might see slightly different versions.</p>
      </div>

    </div>

    <div class="d-grid gap-3 pt-3 mb-4 px-4 bg-light border rounded-3">
      <div class="row">
        <h3>Step 3: Pick Illuminant</h3>
      </div>

      <div class="row pt-2">
        <div class="col-sm-6">
          <select id="whiteSel" style="width: 150px" class="my-2 form-select">
          </select>
          <canvas id="canvasWhite"></canvas>
          <div class="my-2 d-flex justify-content-start"><button id="resetWhite" class="btn btn-primary btn-sm" disabled>Reset Custom Illuminant</button> </div>
          <div><p>You can select an iluminant in the chart above, which shows the SPD of a few CIE Standard Illuminants normalized to peak at unity. You can also draw your own illuminant by selecting "Custom" from the drop-down list.</p></div>
        </div>
        <div class="col-sm-6">
          <p>How do we obtain the XYZ and RGB values of a ColorChecker patch? These patches do not emit lights themselves; they reflect lights. Therefore, we must pick an illuminant for color correction. This inherently suggests that the color correction matrix will be dependent on the illuminant.
          <p>Mathmatically, given an illuminant $\Phi(\lambda)$, the XYZ and the camera raw RGB values of a patch $i$ (with a spectral reflectance of $S_i(\lambda)$) are calculated as:</p>
          <div class="row">
            <div class="col-sm-6">
              <p>$X_i = \sum_{400}^{720}\Phi(\lambda)S_i(\lambda)X(\lambda)$</p>
              <p>$Y_i = \sum_{400}^{720}\Phi(\lambda)S_i(\lambda)Y(\lambda)$</p>
              <p>$Z_i = \sum_{400}^{720}\Phi(\lambda)S_i(\lambda)Z(\lambda)$</p>
            </div>
            <div class="col-sm-6">
              <p>$R_i = \sum_{400}^{720}\Phi(\lambda)S_i(\lambda)R(\lambda)$</p>
              <p>$G_i = \sum_{400}^{720}\Phi(\lambda)S_i(\lambda)G(\lambda)$</p>
              <p>$B_i = \sum_{400}^{720}\Phi(\lambda)S_i(\lambda)B(\lambda)$</p>
            </div>
          </div>
          <p>In practical camera color calibration, instead of applying these equations to calculate the theoretical RGB values, what people usually do is to actually take a photo of the ColorChecker patches and read the raw RGB values. This is necessary because we mostly won't be able to get the exact SPD of the capturing illuminant, and sometimes we don't want to trust the spectral reflectance data. After all, the equations above simulate how RGB values are generated by cameras anyways.</p>
          <p>But if we don't have the illuminant SPD, how do we get the XYZ values of the patches? Fortunately, people have calculated the <a href="https://www.babelcolor.com/colorchecker-2.htm#CCP2_data">XYZ values of the patches</a> under different CIE Standard Illuminants. We would then have to estimate the illuminant of the capturing scene, and find the closest illuminant for which the XYZ values of the patches are available. Estimating the scene illuminant is a difficult topic on its own, and has implications in white balance as well, which we leave for another tutorial.</p>
        </div>
      </div>
    </div>

    <div class="d-grid gap-3 pt-3 mb-4 px-4 bg-light border rounded-3">
      <div class="row">
        <h3>Step 4: Calculate Color Correction Matrix</h3>
        <p>With the illuminant selected, we now have everything to <button id="genLinSys" class="btn btn-primary btn-sm">Generate Equations</button> to concretize the linear system:</p>
        <p id="equText">
        $\begin{bmatrix}
        \boxed{??} \\
        \boxed{??} \\
        \boxed{??}
        \end{bmatrix}$
        $\small{ = \begin{bmatrix}
        T_{00} & T_{01} & T_{02} \\
        T_{10} & T_{11} & T_{12} \\
        T_{20} & T_{21} & T_{22}
        \end{bmatrix} \times}$
        $\begin{bmatrix}
        \boxed{??} \\
        \boxed{??} \\
        \boxed{??}
        \end{bmatrix}$
        </p>
        <p id="equText2">The first matrix has 24 columns, each representing the XYZ values of a ColocChecker patch under your chosen illuminant ${\boxed{??}}$. The last matrix also has 24 columns, each representing the RGB values of a patch in the camera RGB space under the same illuminant. We get an over-determined system with 72 equations and 9 unknowns.</p>
        <p>Before solving the system, let's look at the patches in both the XYZ and the camera RGB spaces, which is shown in the left chart below. Evidently, the XYZ and the RGB values of the patches do not overlap, but if you spin around the chart you'll likely see that the relative distributions of the patches within the XYZ and the RGB space are roughly similar, suggesting that a linear transformation might exist.</p>
      </div>

      <div class="row pt-2">
        <div class="col-sm-6">
          <div id="targetDiv"></div>
        </div>
        <div class="col-sm-6">
          <div id="colDiffDiv"></div>
        </div>
      </div>

      <div class="row pt-2">
        <h4>Solving the Optimization Problem</h4>
        <p>How to find the best fit $T$? The common approach is to formulate this as a <a href="https://en.wikipedia.org/wiki/Linear_least_squares">linear least squares</a> problem, which essentially minimizes the sum of the squared differences between the reconstructed XYZ values and the "ground truth" XYZ values. Critically, squared differences represent Euclidean distances in the XYZ space. However, Euclidean distance in the XYZ space is not proportional to the <a href="https://en.wikipedia.org/wiki/MacAdam_ellipse">perceptual color difference</a>. Therefore, it is usually better to convert the color from the XYZ space to a perceptually uniform color space, such as the <a href="https://en.wikipedia.org/wiki/CIELAB_color_space#CIELAB">CIELAB</a>, before formulating the least squares. For simplicity we won't do it here, but you get the idea.</p>
        <p>The optimal solution to the linear least squares problem in the form of $A=TB$ is: $T=AB^T (BB^T)^{-1}$, where $A$ is our XYZ matrix and $B$ is the RGB matrix. Now click <button id="calcMatrix" class="btn btn-primary btn-sm" disabled>Calculate Matrix</button> to find $T$! The best-fit matrix is calculated as:</p>
        <p id="ccMatText">
        ${\begin{bmatrix}
        \boxed{??} & \boxed{??} & \boxed{??} \\
        \boxed{??} & \boxed{??} & \boxed{??} \\
        \boxed{??} & \boxed{??} & \boxed{??}
        \end{bmatrix}}$
        , for camera ${\boxed{??}}$ and illuminant ${\boxed{??}}$.</p>
        <p>The transformed/corrected XYZ values of the patches will be shown in the chart above. Look for the label "XYZ (Corrected)". You can see that the correct XYZ values pretty much align with the "ground truth" XYZ values. But they don't exactly overlap, because the transformation matrix is just the best fit, not the perfect solution. To see the color correction error, the heatmap on the right shows the distances between the true XYZ and the reproduced XYZ values of the 24 patches. Again, in reality you would estimate the best-fit in a perceptually-uniform space like CIELAB and use a perceptual color difference metric such as <a href="https://en.wikipedia.org/wiki/Color_difference#CIELAB_%CE%94E*">CIE $\Delta E^*$</a>.</p>
        <p id="equText3">This color correction process pretty much what's done in real camera color calibration, which just has a few additional customary normalizations. For instance, when the RGB values are directly read from the raw captures, they are normalized to the range of $[0, 1]$ using the maxium raw value of the sensor. Also, the transformation matrix $T$ is normalized such that the capturing illuminant (${\boxed{??}}$ here), when normalized to have a Y value of 1, saturates one of the RGB channels (usually the G channel is the first to saturate since green filter has the highest sensitivity as is evident in the first chart).</p>
        <h4>Customizing the Camera</h4>
        <p>If you change the camera and the illuminant choice, simply click the "Generate Equations" and "Calculate Matrix" buttons again (in that order). As you change the camera and illuminant, you will see that the correction matrix changes too, confirming that the correction matrix is inherently tied to a particular camera and calibrating illuminant. Here are a few cool things you are invited to try:</p>
        <div>
          <ul>
            <li>Draw camera sensitivity functions that mimic the LMS cone fundamentals or the XYZ CMFs. You will see that the color reproduction error will be relatively small.</li>
            <li>Make each camera sensitivity function narrow and have little overlap across the spectrum.</li>
            <li>Make each camera sensitivity function narrow and have significant overlap.</li>
            <li>Make each camera sensitivity function wide and overlap across the spectrum.</li>
          </ul>
        </div>
      </div>
    </div>

    <div class="d-grid gap-3 pt-3 mb-4 px-4 bg-light border rounded-3">
      <div class="row">
        <h3>Step 5: Interpreting Camera Color Space</h3>
        <p>With the transfromation matrix $T$ calculated, we can use it to get more insights into a particular camera's color space. The TL;DR is that a typical camera's color space contains imaginary colors. While the observations we are about to make are generally true, but let's take one specific camera just so we can be concrete. Click the <button id="chooseCam" class="btn btn-primary btn-sm">Choose iPhone 12 Pro Max</button> button to choose iPhone 12 Pro Max as the target camera and D65 as the illuminant. Everything on this page will be updated accordingly.</p>
        <h4>Are Spectral Lights Corrected?</h4>
        <p>The first question you can ask is, does $T$ correct colors of other lights that are not used for color correction? In particular, we are interested in how the spectral lights are corrected. Go back to the spectral locus plot in Step 1 and click the "Draw Corrected Locus" button, which will plot the transformed spectral locus (the "XYZ (Corrected)" curve). Ideally, you want that locus to precisely match the actual XYZ spectral locus, but most likely you will see that that are way off. This is perhaps not all that surprising, because the spectral lights are not used for calculating the best-fit matrix.</p>
        <p>We can gain a better understanding of the correction results in the chromaticity plot. The chart below plots the "ground truth" spectral locus and the 24 patches in the xy-chromaticity plot, and then overlays the reconstructed locus and patches transformed from the camera RGB space using matrix $T$. Hover on any point will show the coupled counterpart as well. Notice how the original 24 patches and the reconstructed ones are almost overlapped (because they are used to estimate $T$), but the true and corrected spectral locus are miles apart. Those reconstructed spectral colors, when converted to an output-referred color space, will give you incorrect colors.</p>
<!--The way to interpret a point on the corrected locus is that that's what the camera "thinks" what that spectral light's color is (in terms of xy chromaticity). The same interpretation applies to the 24 patches as well.-->
      </div>

      <div class="row d-flex justify-content-center">
        <div class="col-sm-10">
          <div id="chrmDiv"></div>
        </div>
      </div>

      <div class="row pt-2">
        <h4>What's a Camera's Gamut Like?</h4>
        <p>Recall two things here. First, gamut refers to all the colors that can be produced by a color space by mixing (non-negative amount of) the primaries of that color space. Second, the area enclosed by the spectral locus represents all the colors that huiman visual system can see.</p>
        <p>If you click the "sRGB gamut" legend label in the plot above, you will see the gamut of the sRGB color space. It's a triangle inside the spectral locus. The three vertices of the triangle represent the three primaries of the sRGB color space. Clearly, the sRGB priamries are real colors and all the colors produced by the sRGB color space are real colors, too, but there are some real colors that can't produced by sRGB. An interesting observation is that all but one ColorChecker patches lie inside the sRGB gamut. That is intentional. sRGB is the most commonly used color space, and you would want to be able to reproduce its gamut well. By choosing colors within the sRGB gamut as the correction target, it's likely that other colors in the gamut can be reasonably reconstructed as well.</p>
        <p id="camText1">Now click the "Camera RGB gamut" legend label in the plot above. You will see the gamut of ${\boxed{??}}$'s color space, which is still a triangle, because we are still mixing the RGB primaries through a linear combination. This triangle is obtained simply by multiplying $T$ with $[[0, 0, 1], [0, 1, 0], [1, 0, 0]]$ (assuming the raw values are normalized to the range of $[0, 1]$) and converting the results to chromaticities. However, all three primaries are imaginary colors since they lies outside of spectral locus. What this means is that no real photos of iPhone 12 Pro Max will have (normalized) raw RGB values of $[0, 1, 0], [0, 1, 0], [1, 0, 0]$. You can certainly manually set raw pixels to those values if you have a raw image editor, but those values will never be obtained in a real capture.</p>
        <p>You might be wondering, where do real colors reside then? Are they in the area encloused by the corrected/reconstructed spectral locus? Not really. The reason is that the corrected spectral locus no longer has our familiar horseshoe shape and, more importantly, is no longer convex. So, for instance, if you connect spectral lights $520~nm$ and $550~nm$ from the corrected locus, any point on that line represents the color of a real light mixed from $520~nm$ and $550~nm$. If you use iPhone 12 Pro Max to take a photo of a light mixed from $520~nm$ and $550~nm$ and transform the raw RGB values using matrix $T$, you will end up on that line in the xy-chromaticity plot. But that line lies outside the locus. So we can no longer say that real colors lie inside the locus.</p>
        <p id="camText2">In fact, it is the <a href="https://en.wikipedia.org/wiki/Convex_hull">convex hull</a> of the corrected spectral locus that contains the colors of all the real lights. Click the "Convex Hull of Spectral Locus in Camera" legend label to reveal the convex hull. Any point that's outside the convex hull but inside the gamut will have valid (i.e., positive) RGB values, but they will never show up in a real ${\boxed{??}}$ capture because they represent imaginary colors.</p>
      </div>
    </div>

    <div class="b-example-divider"></div>
  </div>


</main>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.1/dist/js/bootstrap.bundle.min.js" integrity="sha384-/bQdsTh/da6pkI1MST/rWKFNjaCP5gBSY4sEBT38Q/9RBh9AH40zEOg7Hlq2THRZ" crossorigin="anonymous"></script>

    <script src="cam.js"></script>
  </body>
</html>
